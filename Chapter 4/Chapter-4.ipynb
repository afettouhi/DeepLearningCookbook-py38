{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from keras.utils import get_file\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "import xml.sax\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import mwparserfromhell\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20200820/',\n",
       " '20200901/',\n",
       " '20200920/',\n",
       " '20201001/',\n",
       " '20201020/',\n",
       " '20201101/',\n",
       " '20201120/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = requests.get('https://dumps.wikimedia.org/enwiki/').text\n",
    "soup_index = BeautifulSoup(index, 'html.parser')\n",
    "dumps = [a['href'] for a in soup_index.find_all('a')\n",
    "             if a.has_attr('href') and a.text[:-1].isdigit()]\n",
    "dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201120/\n"
     ]
    }
   ],
   "source": [
    "for dump_url in sorted(dumps, reverse=True):\n",
    "    print(dump_url)\n",
    "    dump_html = index = requests.get('https://dumps.wikimedia.org/enwiki/' + dump_url).text\n",
    "    soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "    pages_xml = [a['href'] for a in soup_dump.find_all('a')\n",
    "                 if a.has_attr('href') and a['href'].endswith('-pages-articles.xml.bz2')]\n",
    "    if pages_xml:\n",
    "        break\n",
    "    time.sleep(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/af/.keras/datasets/enwiki-20201120-pages-articles.xml.bz2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_dump = pages_xml[0].rsplit('/')[-1]\n",
    "url = url = 'https://dumps.wikimedia.org/' + pages_xml[0]\n",
    "path = get_file(wikipedia_dump, url)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._movies = []\n",
    "        self._curent_tag = None\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._curent_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name in ('title', 'text'):\n",
    "            self._curent_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == self._curent_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            movie = process_article(**self._values)\n",
    "            if movie:\n",
    "                self._movies.append(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_article(title, text):\n",
    "    rotten = [(re.findall('\\d\\d?\\d?%', p),\n",
    "        re.findall('\\d\\.\\d\\/\\d+|$', p), p.lower().find('rotten tomatoes'))\n",
    "        for p in text.split('\\n\\n')]\n",
    "    rating = next(((perc[0], rating[0]) for perc, rating, idx in rotten\n",
    "        if len(perc) == 1 and idx > -1), (None, None))\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    film = next((template for template in wikicode.filter_templates()\n",
    "                 if template.name.strip().lower() == 'infobox film'),\n",
    "                 None)\n",
    "    if film:\n",
    "        properties = {param.name.strip_code().strip():\n",
    "                      param.value.strip_code().strip()\n",
    "                      for param in film.params\n",
    "                      if param.value.strip_code().strip()\n",
    "                     }\n",
    "        links = [x.title.strip_code().strip()\n",
    "                 for x in wikicode.filter_wikilinks()]\n",
    "        return (title, properties, links) + rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parser = xml.sax.make_parser()\n",
    "handler = WikiXmlHandler()\n",
    "parser.setContentHandler(handler)\n",
    "for line in subprocess.Popen(['bzcat'],\n",
    "                             stdin=open(path),\n",
    "                             stdout=subprocess.PIPE).stdout:\n",
    "  try:\n",
    "    parser.feed(line)\n",
    "  except StopIteration:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('wp_movies.ndjson', 'wt') as fout:\n",
    "  for movie in handler._movies:\n",
    "    fout.write(json.dumps(movie) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Reshape\n",
    "from keras.layers.merge import Dot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/wp_movies_10k.ndjson') as fin:\n",
    "    movies = [json.loads(l) for l in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "link_counts = Counter()\n",
    "for movie in movies:\n",
    "    link_counts.update(movie[2])\n",
    "link_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_links = [link for link, c in link_counts.items() if c >= 3]\n",
    "link_to_idx = {link: idx for idx, link in enumerate(top_links)}\n",
    "movie_to_idx = {movie[0]: idx for idx, movie in enumerate(movies)}\n",
    "pairs = []\n",
    "for movie in movies:\n",
    "    pairs.extend((link_to_idx[link], movie_to_idx[movie[0]])\n",
    "                  for link in movie[2] if link in link_to_idx)\n",
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def movie_embedding_model(embedding_size=30):\n",
    "    link = Input(name='link', shape=(1,))\n",
    "    movie = Input(name='movie', shape=(1,))\n",
    "    link_embedding = Embedding(name='link_embedding',\n",
    "        input_dim=len(top_links), output_dim=embedding_size)(link)\n",
    "    movie_embedding = Embedding(name='movie_embedding',\n",
    "        input_dim=len(movie_to_idx), output_dim=embedding_size)(movie)\n",
    "    dot = Dot(name='dot_product', normalize=True, axes=2)(\n",
    "        [link_embedding, movie_embedding])\n",
    "    merged = Reshape((1,))(dot)\n",
    "    model = Model(inputs=[link, movie], outputs=[merged])\n",
    "    model.compile(optimizer='nadam', loss='mse')\n",
    "    return model\n",
    "\n",
    "model = movie_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def batchifier(pairs, positive_samples=50, negative_ratio=5):\n",
    "    batch_size = positive_samples * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    while True:\n",
    "        for idx, (link_id, movie_id) in enumerate(\n",
    "                random.sample(pairs, positive_samples)):\n",
    "            batch[idx, :] = (link_id, movie_id, 1)\n",
    "        idx = positive_samples\n",
    "        while idx < batch_size:\n",
    "            movie_id = random.randrange(len(movie_to_idx))\n",
    "            link_id = random.randrange(len(top_links))\n",
    "            if not (link_id, movie_id) in pairs_set:\n",
    "                batch[idx, :] = (link_id, movie_id, -1)\n",
    "                idx += 1\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'link': batch[:, 0], 'movie': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "positive_samples_per_batch=512\n",
    "\n",
    "model.fit_generator(\n",
    "    batchifier(pairs,\n",
    "               positive_samples=positive_samples_per_batch,\n",
    "               negative_ratio=10),\n",
    "    epochs=25,\n",
    "    steps_per_epoch=len(pairs) // positive_samples_per_batch,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "movie = model.get_layer('movie_embedding')\n",
    "movie_weights = movie.get_weights()[0]\n",
    "lens = np.linalg.norm(movie_weights, axis=1)\n",
    "normalized_movies = (movie_weights.T / lens).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def neighbors(movie):\n",
    "    dists = np.dot(normalized_movies, normalized_movies[movie_to_idx[movie]])\n",
    "    closest = np.argsort(dists)[-10:]\n",
    "    for c in reversed(closest):\n",
    "        print(c, movies[c][0], dists[c])\n",
    "\n",
    "neighbors('Rogue One')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best = ['Star Wars: The Force Awakens', 'The Martian (film)',\n",
    "        'Tangerine (film)', 'Straight Outta Compton (film)',\n",
    "        'Brooklyn (film)', 'Carol (film)', 'Spotlight (film)']\n",
    "worst = ['American Ultra', 'The Cobbler (2014 film)',\n",
    "         'Entourage (film)', 'Fantastic Four (2015 film)',\n",
    "         'Get Hard', 'Hot Pursuit (2015 film)', 'Mortdecai (film)',\n",
    "         'Serena (2014 film)', 'Vacation (2015 film)']\n",
    "y = np.asarray([1 for _ in best] + [0 for _ in worst])\n",
    "X = np.asarray([normalized_movies[movie_to_idx[movie]]\n",
    "                for movie in best + worst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "estimated_movie_ratings = clf.decision_function(normalized_movies)\n",
    "best = np.argsort(estimated_movie_ratings)\n",
    "print('best:')\n",
    "for c in reversed(best[-5:]):\n",
    "    print(c, movies[c][0], estimated_movie_ratings[c])\n",
    "\n",
    "print('worst:')\n",
    "for c in best[:5]:\n",
    "    print(c, movies[c][0], estimated_movie_ratings[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rotten_y = np.asarray([float(movie[-2][:-1]) / 100\n",
    "                       for movie in movies if movie[-2]])\n",
    "rotten_X = np.asarray([normalized_movies[movie_to_idx[movie[0]]]\n",
    "                       for movie in movies if movie[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_CUT_OFF = int(len(rotten_X) * 0.8)\n",
    "regr = LinearRegression()\n",
    "regr.fit(rotten_X[:TRAINING_CUT_OFF], rotten_y[:TRAINING_CUT_OFF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "error = (regr.predict(rotten_X[TRAINING_CUT_OFF:]) -\n",
    "         rotten_y[TRAINING_CUT_OFF:])\n",
    "'mean square error %2.2f' % np.mean(error ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "error = (np.mean(rotten_y[:TRAINING_CUT_OFF]) - rotten_y[TRAINING_CUT_OFF:])\n",
    "'mean square error %2.2f' % np.mean(error ** 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}