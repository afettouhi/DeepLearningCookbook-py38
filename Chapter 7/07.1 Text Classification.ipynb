{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out a simple learner\n",
    "\n",
    "Before we try to build our deep learning models, let's make sure we can learn something using a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.utils.data_utils import get_file\n",
    "import nb_utils\n",
    "\n",
    "emotion_csv = get_file('text_emotion.csv', \n",
    "                       'https://raw.githubusercontent.com/Giohanny/Twitter-Sentiment-Analysis/master/text_emotion.csv')\n",
    "emotion_df = pd.read_csv(emotion_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=VOCAB_SIZE)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X = tfidf_vec.fit_transform(emotion_df['content'])\n",
    "y = label_encoder.fit_transform(emotion_df['sentiment'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2802272727272727"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = MultinomialNB()\n",
    "bayes.fit(X_train, y_train)\n",
    "predictions = bayes.predict(X_test)\n",
    "precision_score(predictions, y_test, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd 0.3293939393939394\n",
      "svm 0.34719696969696967\n",
      "random_forest 0.3297727272727273\n"
     ]
    }
   ],
   "source": [
    "classifiers = {'sgd': SGDClassifier(loss='hinge'),\n",
    "               'svm': SVC(),\n",
    "               'random_forest': RandomForestClassifier()}\n",
    "\n",
    "for lbl, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(lbl, precision_score(predictions, y_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking what our model learned\n",
    "\n",
    "Our linear models appear to be learning something more powerful than \"pick the most popular category\".  We can take a quick look at which words they find the most correlated with each category before moving on to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import eye\n",
    "d = eye(len(tfidf_vec.vocabulary_))\n",
    "word_pred = bayes.predict_proba(d)\n",
    "inverse_vocab = {idx: word for word, idx in tfidf_vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "by_cls = defaultdict(Counter)\n",
    "for word_idx, pred in enumerate(word_pred):\n",
    "    for class_idx, score in enumerate(pred):\n",
    "        cls = label_encoder.classes_[class_idx]\n",
    "        by_cls[cls][inverse_vocab[word_idx]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in by_cls:\n",
    "    words = [x[0] for x in by_cls[k].most_common(5)]\n",
    "    print(k, ':', ' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a deep model\n",
    "\n",
    "Now that we've seen how well a simple linear model can do, let's see if we can do any better with a deep learning model.  In this case, we don't have an excessive amount of training data: this constrains the models we can train effectively: use too big of a model, and we'll end up overfitting our data.\n",
    "\n",
    "We'll start with a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "chars = list(sorted(set(chain(*emotion_df['content']))))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "max_sequence_len = max(len(x) for x in emotion_df['content'])\n",
    "\n",
    "char_vectors = []\n",
    "for txt in emotion_df['content']:\n",
    "    vec = np.zeros((max_sequence_len, len(char_to_idx)))\n",
    "    vec[np.arange(len(txt)), [char_to_idx[ch] for ch in txt]] = 1\n",
    "    char_vectors.append(vec)\n",
    "char_vectors = np.asarray(char_vectors)\n",
    "char_vectors = pad_sequences(char_vectors)\n",
    "labels = label_encoder.transform(emotion_df['sentiment'])\n",
    "\n",
    "\n",
    "def split(lst):\n",
    "    training_count = int(0.9 * len(char_vectors))\n",
    "    return lst[:training_count], lst[training_count:]\n",
    "\n",
    "training_char_vectors, test_char_vectors = split(char_vectors)\n",
    "training_labels, test_labels = split(labels)\n",
    "\n",
    "char_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='input')\n",
    "    \n",
    "    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "    max_pool_1x = MaxPooling1D(6)(conv_1x)\n",
    "    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "    max_pool_2x = MaxPooling1D(6)(conv_2x)\n",
    "\n",
    "    flatten = Flatten()(max_pool_2x)\n",
    "    dense = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
    "char_cnn_model.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='input')\n",
    "    \n",
    "    layers = []\n",
    "    for window in (5, 6, 7):\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)\n",
    "        max_pool_1x = MaxPooling1D(window)(conv_1x)\n",
    "        dropout_1x = Dropout(0.3)(max_pool_1x)\n",
    "        conv_2x = Conv1D(128, window, activation='relu', padding='valid')(dropout_1x)\n",
    "        max_pool_2x = MaxPooling1D(window)(conv_2x)\n",
    "        dropout_2x = Dropout(0.3)(max_pool_2x)\n",
    "        layers.append(dropout_2x)\n",
    "\n",
    "    if len(layers) > 1:\n",
    "        merged = Concatenate(axis=1)(layers)\n",
    "    else:\n",
    "        merged = layers[0]\n",
    "\n",
    "    dropout = Dropout(0.3)(merged)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
    "char_cnn_model.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing and preparing our data\n",
    "\n",
    "Just like we did when computing word embeddings, we want to featurize our data so we can classify it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(emotion_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This may take a while to load\n",
    "\n",
    "w2v, idf = nb_utils.load_w2v(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(emotion_df['content'])\n",
    "tokens = pad_sequences(tokens)\n",
    "\n",
    "\n",
    "\n",
    "training_tokens, training_labels = tokens[:training_count], labels[:training_count]\n",
    "test_tokens, test_labels = tokens[training_count:], labels[training_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def make_embedding(name, vocab_size, embedding_size, weights=None, mask_zero=True):\n",
    "    if weights is not None:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=weights.shape[1], \n",
    "                                weights=[weights], trainable=False, \n",
    "                                name='%s/embedding' % name)\n",
    "    else:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=embedding_size,\n",
    "                                name='%s/embedding' % name)\n",
    "\n",
    "def create_unigram_model(vocab_size, embedding_size=None, embedding_weights=None, idf_weights=None):\n",
    "    assert not (embedding_size is None and embedding_weights is None)\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='message')\n",
    "    \n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)\n",
    "    idf = make_embedding('message_idf', vocab_size, embedding_size, idf_weights)\n",
    "\n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_and_sum(args):\n",
    "        embedding, idf = args\n",
    "        return K.sum(embedding * K.abs(idf), axis=1)\n",
    "\n",
    "    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')\n",
    "    sum_msg = sum_layer([mask(embedding(message)), idf(message)])\n",
    "    fc1 = layers.Dense(units=128, activation='relu')(sum_msg)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=categories,\n",
    "    )\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "unigram_model = create_unigram_model(vocab_size=VOCAB_SIZE,\n",
    "                                     embedding_weights=w2v,\n",
    "                                     idf_weights=idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model.fit(training_tokens, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Embeddings\n",
    "\n",
    "It looks like our model with pre-trained embeddings isn't doing much better than the linear models.\n",
    "\n",
    "We can also try training a model \"from scratch\", and learn the word embeddings from our training data.  Note that we use a small embedding size here to speed up training and to try to avoid overfitting.\n",
    "\n",
    "Only training for 10 epochs stops the model while it is still improving on the training set, but prevents it\n",
    "from overfitting.  We can formalize this by using a validation set and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_embeddings_model = create_unigram_model(vocab_size=VOCAB_SIZE, embedding_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_embeddings_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the test set accuracy is lower than that on the training set.\n",
    "\n",
    "learned_embeddings_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complex Models\n",
    "\n",
    "As with our previous task, we can try using more powerful models to classify our text.  In this case, the limited training data and text size limit their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    \n",
    "    # The convolution layer in keras does not support masking, so we just allow\n",
    "    # the embedding layer to learn an explicit value.\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights,\n",
    "                              mask_zero=False)\n",
    "\n",
    "    def _combine_sum(v):\n",
    "        return K.sum(v, axis=1)\n",
    "\n",
    "    cnn_1 = layers.Convolution1D(128, 3)\n",
    "    cnn_2 = layers.Convolution1D(128, 3)\n",
    "    cnn_3 = layers.Convolution1D(128, 3)\n",
    "    \n",
    "    global_pool = layers.GlobalMaxPooling1D()\n",
    "    local_pool = layers.MaxPooling1D(strides=1, pool_size=3)\n",
    "\n",
    "    cnn_encoding = global_pool(cnn_3(local_pool(cnn_2(local_pool(cnn_1(embedding(message)))))))\n",
    "    fc1 = layers.Dense(units=128, activation='elu')(cnn_encoding)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[categories],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(training_tokens, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)(message)\n",
    "\n",
    "    lstm_1 = layers.LSTM(units=128, return_sequences=False)(embedding)\n",
    "#     lstm_2 = layers.LSTM(units=128, return_sequences=False)(lstm_1)\n",
    "    category = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(lstm_1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[category],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our models\n",
    "\n",
    "Let's compare the predictions from our models on a sample of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'lstm': lstm_model.predict(test_tokens[:100]),\n",
    "    'char_cnn': char_cnn_model.predict(test_char_vectors[:100]),\n",
    "    'cnn': cnn_model.predict(test_tokens[:100]),\n",
    "    'unigram': unigram_model.predict(test_tokens[:100]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe just for test data\n",
    "\n",
    "pd.options.display.max_colwidth = 128\n",
    "test_df = emotion_df[training_count:training_count+100].reset_index()\n",
    "eval_df = pd.DataFrame({\n",
    "    'content': test_df['content'],\n",
    "    'true': test_df['sentiment'],\n",
    "    'lstm': [label_encoder.classes_[np.argmax(x)] for x in predictions['lstm']],\n",
    "    'cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['cnn']],\n",
    "    'char_cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['char_cnn']],    \n",
    "    'unigram': [label_encoder.classes_[np.argmax(x)] for x in predictions['unigram']],\n",
    "})\n",
    "eval_df = eval_df[['content', 'true', 'lstm', 'cnn', 'char_cnn', 'unigram']]\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation\n",
    "\n",
    "We can examine some of our error cases by hand.  Often, the models tend to agree when they make mistakes, and that the mistakes aren't unreasonable: this task would be challenging even for a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Tweets\n",
    "\n",
    "We can gather a sample of Twitter data using the Twitter API (https://dev.twitter.com).  To do so, we'll need to create a Twitter application and get credentials for it.  You can do this manually at https://app.twitter.com.  Once you have an app, go to the \"Key and Access Tokens\" tab to find your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill these in!\n",
    "\n",
    "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
    "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
    "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
    "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api = twitter.Twitter(\n",
    "    auth=twitter.OAuth(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        token=ACCESS_TOKEN,\n",
    "        token_secret=ACCESS_SECRET,\n",
    "    ))\n",
    "\n",
    "stream = twitter.TwitterStream(\n",
    "    auth=twitter.OAuth(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        token=ACCESS_TOKEN,\n",
    "        token_secret=ACCESS_SECRET,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def has_emoji(tweet):\n",
    "    if tweet.get('lang') != 'en':\n",
    "        return False\n",
    "    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n",
    "\n",
    "%time st = list(itertools.islice(filter(has_emoji, stream.statuses.sample()), 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(st), [t.get('text', None) for t in st][:10]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}