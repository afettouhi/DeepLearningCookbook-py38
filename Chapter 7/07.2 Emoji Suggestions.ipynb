{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Tweets\n",
    "\n",
    "We can gather a sample of Twitter data using the Twitter API (https://dev.twitter.com).  To do so, we'll need to create a Twitter application and get credentials for it.  You can do this manually at https://app.twitter.com.  Once you have an app, go to the \"Key and Access Tokens\" tab to find your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import twitter\n",
    "import emoji\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "import keras.callbacks\n",
    "import json\n",
    "\n",
    "import os\n",
    "import nb_utils\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Merge, LSTM, Embedding, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate, Average\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill these in!\n",
    "\n",
    "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
    "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
    "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
    "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth=twitter.OAuth(\n",
    "    consumer_key=CONSUMER_KEY,\n",
    "    consumer_secret=CONSUMER_SECRET,\n",
    "    token=ACCESS_TOKEN,\n",
    "    token_secret=ACCESS_SECRET,\n",
    ")\n",
    "\n",
    "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
    "\n",
    "[x['text'] for x in itertools.islice(status_stream.sample(), 0, 5) if x.get('text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
    "\n",
    "def english_has_emoji(tweet):\n",
    "    if tweet.get('lang') != 'en':\n",
    "        return False\n",
    "    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n",
    "\n",
    "%time tweets = list(itertools.islice(filter(english_has_emoji, status_stream.sample()), 0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped = []\n",
    "for tweet in tweets:\n",
    "    text = tweet['text']\n",
    "    emojis = {ch for ch in text if ch in emoji.UNICODE_EMOJI}\n",
    "    if len(emojis) == 1:\n",
    "        emoiji = emojis.pop()\n",
    "        text = ''.join(ch for ch in text if ch != emoiji)\n",
    "        stripped.append((text, emoiji))\n",
    "len(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CNN\n",
    "\n",
    "Let's see what the CNN of the previous chapter does on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv('data/emojis.csv')\n",
    "all_tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = all_tweets.groupby('emoji').filter(lambda c:len(c) > 1000)\n",
    "tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tweets['text'], key=lambda t:len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sorted(set(chain(*tweets['text']))))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "max_sequence_len = max(len(x) for x in tweets['text'])\n",
    "\n",
    "emojis = list(sorted(set(tweets['emoji'])))\n",
    "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
    "emojis[:10]\n",
    "\n",
    "train_tweets, test_tweets = train_test_split(tweets, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(tweets, batch_size):\n",
    "    while True:\n",
    "        if batch_size is None:\n",
    "            batch = tweets\n",
    "            batch_size = batch.shape[0]\n",
    "        else:\n",
    "            batch = tweets.sample(batch_size)\n",
    "        X = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
    "        y = np.zeros((batch_size,))\n",
    "        for row_idx, (_, row) in enumerate(batch.iterrows()):\n",
    "            y[row_idx] = emoji_to_idx[row['emoji']]\n",
    "            for ch_idx, ch in enumerate(row['text']):\n",
    "                X[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
    "        yield X, y\n",
    "\n",
    "next(data_generator(tweets, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
    "    \n",
    "    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "    max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
    "    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "    max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
    "\n",
    "    flatten = Flatten()(max_pool_2x)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), max_sequence_len, len(emojis))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=0.03,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "char_cnn_model.fit_generator(\n",
    "    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model.evaluate_generator(\n",
    "    data_generator(test_tweets, batch_size=BATCH_SIZE),\n",
    "    steps=len(test_tweets) / BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('zoo/07/emoji_chars.json', 'w') as fout:\n",
    "    json.dump({\n",
    "        'emojis': ''.join(emojis),\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'max_sequence_len': max_sequence_len,\n",
    "    }, fout)\n",
    "char_cnn_model.save('zoo/07/char_cnn_model.h5')\n",
    "char_cnn_model.save_weights('zoo/07/char_cnn_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 128\n",
    "inspect_tweets = test_tweets.sample(100)\n",
    "predicted = char_cnn_model.predict_generator(data_generator(inspect_tweets, batch_size=None), steps=1)\n",
    "show = pd.DataFrame({\n",
    "    'text': inspect_tweets['text'],\n",
    "    'true': inspect_tweets['emoji'],\n",
    "    'pred': [emojis[np.argmax(x)] for x in predicted],\n",
    "})\n",
    "show = show[['text', 'true', 'pred']]\n",
    "show.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Merge, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "def create_char_cnn_model2(num_chars, max_sequence_len, num_labels, drop_out=0.25):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
    "    \n",
    "    layers = []\n",
    "    for window in (4, 5, 6):\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)\n",
    "        max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
    "        dropout_1x = Dropout(drop_out)(max_pool_1x)\n",
    "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(dropout_1x)\n",
    "        max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
    "        dropout_2x = Dropout(drop_out)(max_pool_2x)\n",
    "        layers.append(dropout_2x)\n",
    "\n",
    "    merged = Concatenate(axis=1)(layers)\n",
    "\n",
    "    dropout = Dropout(drop_out)(merged)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model2 = create_char_cnn_model2(len(char_to_idx), max_sequence_len, len(emojis))\n",
    "char_cnn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "char_cnn_model2.fit_generator(\n",
    "    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
    "    epochs=30,\n",
    "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model2.evaluate_generator(\n",
    "    data_generator(test_tweets, batch_size=BATCH_SIZE),\n",
    "    steps=len(test_tweets) / BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"75s - loss: 2.3855 - acc: 0.4368\\n[2.8089022636413574, 0.38840296648550726]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing and preparing our data\n",
    "\n",
    "Just like we did when computing word embeddings, we want to featurize our data so we can classify it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_tokens = tokenizer.texts_to_sequences(train_tweets['text'])\n",
    "test_tokens = tokenizer.texts_to_sequences(test_tweets['text'])\n",
    "max_num_tokens = max(len(x) for x in chain(training_tokens, test_tokens))\n",
    "training_tokens = pad_sequences(training_tokens, maxlen=max_num_tokens)\n",
    "test_tokens = pad_sequences(test_tokens, maxlen=max_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_labels = np.asarray([emoji_to_idx[em] for em in train_tweets['emoji']])\n",
    "test_labels = np.asarray([emoji_to_idx[em] for em in test_tweets['emoji']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(tokenizer):\n",
    "    model = Word2Vec.load('data/twitter_w2v.model')\n",
    "    w2v = np.zeros((tokenizer.num_words, w2v_model.syn0.shape[1]))\n",
    "    for k, v in tokenizer.word_index.items():\n",
    "        if v >= tokenizer.num_words:\n",
    "            continue\n",
    "        if k in w2v_model:\n",
    "            w2v[v] = w2v_model[k]\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while to load\n",
    "\n",
    "#w2v = load_weights(tokenizer)\n",
    "\n",
    "#model = Word2Vec.load('data/twitter_w2v.model')\n",
    "w2v = np.zeros((tokenizer.num_words, model.wv.syn0.shape[1]))\n",
    "found = 0\n",
    "for k, v in tokenizer.word_index.items():\n",
    "    if v >= tokenizer.num_words:\n",
    "        continue\n",
    "    if k in model:\n",
    "        w2v[v] = model[k]\n",
    "        found += 1\n",
    "found, tokenizer.num_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Level CNN\n",
    "\n",
    "As with our previous task, we can try using more powerful models to classify our text.  In this case, the limited training data and text size limit their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None, drop_out=0.2):\n",
    "    message = Input(shape=(max_num_tokens,), dtype='int32', name='cnn_input')\n",
    "    \n",
    "    \n",
    "    # The convolution layer in keras does not support masking, so we just allow\n",
    "    # the embedding layer to learn an explicit value.\n",
    "    embedding = Embedding(mask_zero=False, input_dim=vocab_size, \n",
    "                          output_dim=embedding_weights.shape[1], \n",
    "                          weights=[embedding_weights],\n",
    "                          trainable=True,\n",
    "                          name='cnn_embedding')(message)\n",
    "    \n",
    "    global_pools = []\n",
    "    for window in 2, 3:\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(embedding)\n",
    "        max_pool_1x = MaxPooling1D(2)(conv_1x)\n",
    "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_1x)\n",
    "        max_pool_2x = MaxPooling1D(2)(conv_2x)\n",
    "        conv_3x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_2x)\n",
    "\n",
    "        global_pools.append(GlobalMaxPooling1D()(conv_3x))\n",
    "\n",
    "    merged = Concatenate(axis=1)(global_pools)\n",
    "    fc1 = Dense(units=128, activation='elu')(merged)\n",
    "    preds = Dense(units=len(emojis), activation='softmax', name='cnn_predictions')(fc1)\n",
    "    model = Model(\n",
    "        inputs=[message],\n",
    "        outputs=[preds],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(training_tokens, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = Input(shape=(None,), dtype='int32', name='lstm_input')\n",
    "    embedding = Embedding(mask_zero=False, input_dim=vocab_size, \n",
    "                          output_dim=embedding_weights.shape[1], \n",
    "                          weights=[embedding_weights],\n",
    "                          trainable=True,\n",
    "                          name='lstm_embedding')(message)\n",
    "\n",
    "    lstm_1 = LSTM(units=128, return_sequences=False)(embedding)\n",
    "    preds = Dense(units=len(emojis), activation='softmax', name='lstm_predictions')(lstm_1)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[message],\n",
    "        outputs=[preds],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(training_tokens, training_labels, epochs=12, batch_size=1024, callbacks=[early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our models\n",
    "\n",
    "Let's compare the predictions from our models on a sample of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_char_vectors, _ = next(data_generator(test_tweets, None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    label: [emojis[np.argmax(x)] for x in pred]\n",
    "    for label, pred in (\n",
    "        ('lstm', lstm_model.predict(test_tokens[:100])),\n",
    "        ('char_cnn', char_cnn_model.predict(test_char_vectors[:100])),\n",
    "        ('cnn', cnn_model.predict(test_tokens[:100])),\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe just for test data\n",
    "pd.options.display.max_colwidth = 128\n",
    "test_df = test_tweets[:100].reset_index()\n",
    "eval_df = pd.DataFrame({\n",
    "    'content': test_df['text'],\n",
    "    'true': test_df['emoji'],\n",
    "    **predictions\n",
    "})\n",
    "eval_df[['content', 'true', 'char_cnn', 'cnn', 'lstm']].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation\n",
    "\n",
    "We can examine some of our error cases by hand.  Often, the models tend to agree when they make mistakes, and that the mistakes aren't unreasonable: this task would be challenging even for a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_data_generator(tweets, tokens, batch_size):\n",
    "    tweets = tweets.reset_index()\n",
    "    while True:\n",
    "        batch_idx = random.sample(range(len(tweets)), batch_size)\n",
    "        tweet_batch = tweets.iloc[batch_idx]\n",
    "        token_batch = tokens[batch_idx]\n",
    "        char_vec = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
    "        token_vec = np.zeros((batch_size, max_num_tokens))\n",
    "        y = np.zeros((batch_size,))\n",
    "        for row_idx, (token_row, (_, tweet_row)) in enumerate(zip(token_batch, tweet_batch.iterrows())):\n",
    "            y[row_idx] = emoji_to_idx[tweet_row['emoji']]\n",
    "            for ch_idx, ch in enumerate(tweet_row['text']):\n",
    "                char_vec[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
    "            token_vec[row_idx, :] = token_row\n",
    "        yield {'char_cnn_input': char_vec, 'cnn_input': token_vec, 'lstm_input': token_vec}, y\n",
    "\n",
    "d, y = next(combined_data_generator(train_tweets, training_tokens, 5))\n",
    "d['lstm_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_layer(model):\n",
    "    layers = [layer for layer in model.layers if layer.name.endswith('_predictions')]\n",
    "    return layers[0].output\n",
    "\n",
    "def create_ensemble(*models):\n",
    "    inputs = [model.input for model in models]\n",
    "    predictions = [prediction_layer(model) for model in models]\n",
    "    merged = Average()(predictions)\n",
    "    model = Model(\n",
    "        inputs=inputs,\n",
    "        outputs=[merged],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "ensemble = create_ensemble(char_cnn_model2, cnn_model, lstm_model)\n",
    "ensemble.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "ensemble.fit_generator(\n",
    "    combined_data_generator(train_tweets, training_tokens, BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.evaluate_generator(\n",
    "    combined_data_generator(test_tweets, test_tokens, BATCH_SIZE),\n",
    "    steps=len(test_tweets) / BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}