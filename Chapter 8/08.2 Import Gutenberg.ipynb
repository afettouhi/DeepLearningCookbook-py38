{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing gutenberg is tricky, so it is not included in the requirements.txt\n",
    "# For it to work we need a berkelydb version <= 6 for licensing reasons. On OSX\n",
    "# using brew you can do:\n",
    "#   brew install berkeley-db@4\n",
    "\n",
    "# !pip install gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, defaultdict\n",
    "from gutenberg.acquire.text import UnknownDownloadUriException\n",
    "import re\n",
    "from gensim.utils import tokenize\n",
    "import random\n",
    "import nltk\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('Twain, Mark', 1835, 210),\n ('Ebers, Georg', 1837, 164),\n ('Parker, Gilbert', 1862, 135),\n ('Fenn, George Manville', 1831, 128),\n ('Jacobs, W. W. (William Wymark)', 1863, 112)]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/gutenberg_index.json') as fin:\n",
    "    authors = json.load(fin)\n",
    "recent = [x for x in authors if 'birthdate' in x and x['birthdate'] > 1830]\n",
    "[(x['name'], x['birthdate'], x['english_books']) for x in recent[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list_supported_metadatas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1126"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAGRAPH_SPLIT_RE = re.compile(r'\\n *\\n+')\n",
    "\n",
    "def extract_conversations(text, quote='\"'):\n",
    "    paragraphs = PARAGRAPH_SPLIT_RE.split(text.strip())\n",
    "    conversations = [['']]\n",
    "    for paragraph in paragraphs:\n",
    "        chunks = paragraph.replace('\\n', ' ').split(quote)\n",
    "        for i in range((len(chunks) + 1) // 2):\n",
    "            if (len(chunks[i * 2]) > 100 or len(chunks) == 1) and conversations[-1] != ['']:\n",
    "                if conversations[-1][-1] == '':\n",
    "                    del conversations[-1][-1]\n",
    "                conversations.append([''])\n",
    "            if i * 2 + 1 < len(chunks):\n",
    "                chunk = chunks[i * 2 + 1]\n",
    "                if chunk:\n",
    "                    if conversations[-1][-1]:\n",
    "                        if chunk[0] >= 'A' and chunk[0] <= 'Z':\n",
    "                            if conversations[-1][-1].endswith(','):\n",
    "                                conversations[-1][-1] = conversations[-1][-1][:-1]\n",
    "                            conversations[-1][-1] += '.'\n",
    "                        conversations[-1][-1] += ' '\n",
    "                    conversations[-1][-1] += chunk\n",
    "        if conversations[-1][-1]:\n",
    "            conversations[-1].append('')\n",
    "\n",
    "    return [x for x in conversations if len(x) > 1]\n",
    "\n",
    "\n",
    "conversations = extract_conversations(strip_headers(load_etext(10008).strip()))\n",
    "sum(len(x) for x in conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-2107672d3d5b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0mbooks\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             \u001B[0mtxt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstrip_headers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mload_etext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbook\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mUnknownDownloadUriException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/DeepLearningCookbook-py38/lib/python3.8/site-packages/gutenberg/acquire/text.py\u001B[0m in \u001B[0;36mload_etext\u001B[0;34m(etextno, refresh_cache, mirror, prefer_ascii)\u001B[0m\n\u001B[1;32m    147\u001B[0m         \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mclosing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgzip\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcached\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'w'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcache\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 149\u001B[0;31m             \u001B[0mcache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'utf-8'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mclosing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgzip\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcached\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcache\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/DeepLearningCookbook-py38/lib/python3.8/gzip.py\u001B[0m in \u001B[0;36mwrite\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    279\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlength\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 280\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfileobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompress\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompress\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    281\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mlength\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    282\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcrc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mzlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcrc32\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcrc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "LATIN_1_CHARS = (\n",
    "    (u'\\xe2\\x80\\x99', \"'\"),\n",
    "    (u'\\xc3\\xa9', 'e'),\n",
    "    (u'\\xe2\\x80\\x90', '-'),\n",
    "    (u'\\xe2\\x80\\x91', '-'),\n",
    "    (u'\\xe2\\x80\\x92', '-'),\n",
    "    (u'\\xe2\\x80\\x93', '-'),\n",
    "    (u'\\xe2\\x80\\x94', '-'),\n",
    "    (u'\\xe2\\x80\\x94', '-'),\n",
    "    (u'\\xe2\\x80\\x98', \"'\"),\n",
    "    (u'\\xe2\\x80\\x9b', \"'\"),\n",
    "    (u'\\xe2\\x80\\x9c', '\"'),\n",
    "    (u'\\xe2\\x80\\x9c', '\"'),\n",
    "    (u'\\xe2\\x80\\x9d', '\"'),\n",
    "    (u'\\xe2\\x80\\x9e', '\"'),\n",
    "    (u'\\xe2\\x80\\x9f', '\"'),\n",
    "    (u'\\xe2\\x80\\xa6', '...'),\n",
    "    (u'\\xe2\\x80\\xb2', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb3', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb4', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb5', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb6', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb7', \"'\"),\n",
    "    (u'\\xe2\\x81\\xba', \"+\"),\n",
    "    (u'\\xe2\\x81\\xbb', \"-\"),\n",
    "    (u'\\xe2\\x81\\xbc', \"=\"),\n",
    "    (u'\\xe2\\x81\\xbd', \"(\"),\n",
    "    (u'\\xe2\\x81\\xbe', \")\")\n",
    ")\n",
    "\n",
    "books = 0\n",
    "for author in recent[:1000]:\n",
    "    for book in author['books']:\n",
    "        books += 1\n",
    "        try:\n",
    "            txt = strip_headers(load_etext(int(book[0]))).strip()\n",
    "        except UnknownDownloadUriException:\n",
    "            continue\n",
    "        for ch1, ch2 in LATIN_1_CHARS:\n",
    "            txt = txt.replace(ch1, ch2)\n",
    "        conversations += extract_conversations(txt)\n",
    "\n",
    "print(len(conversations), books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gutenberg.txt', 'w') as fout:\n",
    "    for conv in conversations:\n",
    "        fout.write('\\n'.join(conv) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_TOKEN = re.compile('(\\w+|\\?)', re.UNICODE)\n",
    "token_counter = Counter()\n",
    "with open('gutenberg.txt') as fin:\n",
    "    for line in fin:\n",
    "        line = line.lower().replace('_', ' ')\n",
    "        token_counter.update(RE_TOKEN.findall(line))\n",
    "with open('gutenberg.tok', 'w') as fout:\n",
    "    for token, count in token_counter.items():\n",
    "        fout.write('%s\\t%d\\n' % (token, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_ALPHABETIC.findall(conv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}