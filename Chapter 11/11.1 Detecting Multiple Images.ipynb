{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imread' from 'scipy.misc' (/home/af/Dokumenter/Programs/miniconda3/envs/DeepLearningCookbook-py38/lib/python3.8/site-packages/scipy/misc/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-7e76f7e44e32>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mPIL\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mImageDraw\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmisc\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mimread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimresize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimsave\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromimage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtoimage\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'imread' from 'scipy.misc' (/home/af/Dokumenter/Programs/miniconda3/envs/DeepLearningCookbook-py38/lib/python3.8/site-packages/scipy/misc/__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Flatten, Dense, Input, TimeDistributed\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from keras.preprocessing import image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from scipy.misc import imread, imresize, imsave, fromimage, toimage\n",
    "\n",
    "try:\n",
    "    from io import BytesIO\n",
    "except ImportError:\n",
    "    from StringIO import StringIO as BytesIO\n",
    "import PIL\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(a, fmt='jpeg'):\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "\n",
    "def preprocess_image(image_path, target_size=None):\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x, w, h):\n",
    "    x = x.copy()\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, w, h))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((w, h, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dog = preprocess_image('../data/cat_dog.jpg', target_size=(224, 224))\n",
    "preds = base_model.predict(cat_dog)\n",
    "print('Predicted:', vgg16.decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dog_img = image.load_img('../data/cat_dog.jpg', target_size=(448, 448))\n",
    "draw = ImageDraw.Draw(cat_dog_img)\n",
    "draw.rectangle((192, 96, 416, 320), outline=(0, 0, 0))\n",
    "draw.rectangle((0, 192, 224, 416), outline=(0, 0, 0))\n",
    "cat_dog_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dog2 = preprocess_image('../data/cat_dog.jpg', target_size=(448, 448))\n",
    "showarray(deprocess_image(cat_dog2, 448, 448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = []\n",
    "for x in range(7):\n",
    "    for y in range(7):\n",
    "        crops.append(cat_dog2[0, x * 32: x * 32 + 224, y * 32: y * 32 + 224, :])\n",
    "crops = np.asarray(crops)\n",
    "showarray(deprocess_image(crops[0], 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = base_model.predict(vgg16.preprocess_input(crops))\n",
    "crop_scores = defaultdict(list)\n",
    "for idx, pred in enumerate(vgg16.decode_predictions(preds, top=1)):\n",
    "    _, label, weight = pred[0]\n",
    "    crop_scores[label].append((idx, weight))\n",
    "crop_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_image_for_label(l, label):\n",
    "    idx = max(l[label], key=lambda t:t[1])[0]\n",
    "    return deprocess_image(crops[idx], 224, 224)\n",
    "\n",
    "showarray(best_image_for_label(crop_scores, 'Egyptian_cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showarray(best_image_for_label(crop_scores, 'Labrador_retriever'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_model(base_model):\n",
    "    inputs = Input(shape=(7, 7, 512), name='input')\n",
    "    flatten = Flatten(name='flatten')(inputs)\n",
    "    fc1 = Dense(4096, activation='relu', name='fc1')(flatten)\n",
    "    fc2 = Dense(4096, activation='relu', name='fc2')(fc1)\n",
    "    predictions = Dense(1000, activation='softmax', name='predictions')(fc2)\n",
    "    model = Model(inputs, predictions, name='top_model')\n",
    "    for layer in model.layers:\n",
    "        if layer.name != 'input':\n",
    "            print(layer.name)\n",
    "            layer.set_weights(base_model.get_layer(layer.name).get_weights())\n",
    "    return model\n",
    "\n",
    "top_model = create_top_model(base_model)\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "bottom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = base_model.predict(crops[:1])\n",
    "vgg16.decode_predictions(p0, top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = bottom_model.predict(crops[:1])\n",
    "t0 = top_model.predict(b0[:, :, :, :])\n",
    "vgg16.decode_predictions(t0, top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_out = bottom_model.predict(cat_dog2)\n",
    "bottom_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_crops = []\n",
    "for x in range(7):\n",
    "    for y in range(7):\n",
    "        vec_crops.append(bottom_out[0, x: x + 7, y: y + 7, :])\n",
    "vec_crops = np.asarray(vec_crops)\n",
    "vec_crops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = top_model.predict(vec_crops[:1])\n",
    "vgg16.decode_predictions(t0, top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_pred = top_model.predict(vec_crops)\n",
    "l = defaultdict(list)\n",
    "for idx, pred in enumerate(vgg16.decode_predictions(crop_pred, top=1)):\n",
    "    _, label, weight = pred[0]\n",
    "    l[label].append((idx, weight))\n",
    "l.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showarray(best_image_for_label(l, 'golden_retriever'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showarray(best_image_for_label(l, 'tabby'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../../checkout/keras-frcnn')\n",
    "import keras_frcnn, keras_frcnn.roi_helpers\n",
    "\n",
    "import pickle\n",
    "c = pickle.load(open('data/config.pickle', 'rb'))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras_frcnn.resnet as nn\n",
    "\n",
    "num_features = 1024\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    input_shape_img = (3, None, None)\n",
    "    input_shape_features = (num_features, None, None)\n",
    "else:\n",
    "    input_shape_img = (None, None, 3)\n",
    "    input_shape_features = (None, None, num_features)\n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(c.num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "# define the base network (resnet here, can be VGG, Inception, etc)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(c.anchor_box_scales) * len(c.anchor_box_ratios)\n",
    "rpn_layers = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(feature_map_input, roi_input, c.num_rois, nb_classes=len(c.class_mapping), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier_only = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_rpn.load_weights('data/model_frcnn.hdf5', by_name=True)\n",
    "model_classifier.load_weights('data/model_frcnn.hdf5', by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../data/cat_dog.jpg')\n",
    "\n",
    "X, ratio = format_img(img, c)\n",
    "\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "y1, y2, f = model_rpn.predict(X)\n",
    "r = keras_frcnn.roi_helpers.rpn_to_roi(y1, y2, c, K.image_dim_ordering(), overlap_thresh=0.7)\n",
    "roi_count = R.shape[0] // c.num_rois\n",
    "r2 = np.zeros((roi_count * c.num_rois, r.shape[1]))\n",
    "r2 = r[:r2.shape[0],:r2.shape[1]]\n",
    "r2 = np.reshape(r2, (roi_count, c.num_rois, r.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {v: k for k, v in c.class_mapping.items()}\n",
    "bboxes = []\n",
    "\n",
    "C = c\n",
    "for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "\t\tROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "\t\tif ROIs.shape[1] == 0:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tif jk == R.shape[0]//C.num_rois:\n",
    "\t\t\t#pad R\n",
    "\t\t\tcurr_shape = ROIs.shape\n",
    "\t\t\ttarget_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "\t\t\tROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "\t\t\tROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "\t\t\tROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "\t\t\tROIs = ROIs_padded\n",
    "\n",
    "\t\t[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "\t\tfor ii in range(P_cls.shape[1]):\n",
    "\n",
    "\t\t\tif np.max(P_cls[0, ii, :]) < 0.8 or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tcls_name = idx_to_class[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "\t\t\t(x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "\t\t\tcls_num = np.argmax(P_cls[0, ii, :])\n",
    "\t\t\ttry:\n",
    "\t\t\t\t(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "\t\t\t\ttx /= C.classifier_regr_std[0]\n",
    "\t\t\t\tty /= C.classifier_regr_std[1]\n",
    "\t\t\t\ttw /= C.classifier_regr_std[2]\n",
    "\t\t\t\tth /= C.classifier_regr_std[3]\n",
    "\t\t\t\tx, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t\tbboxes.append((\n",
    "                cls_name, np.max(P_cls[0, ii, :]), C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)\n",
    "                                ))\n",
    "del C\n",
    "                           \n",
    "bboxes =  list(sorted(bboxes, key=lambda t:t[1], reverse=True))\n",
    "print(bboxes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = keras_frcnn.roi_helpers.rpn_to_roi(Y1, Y2, c, K.image_dim_ordering(), overlap_thresh=0.7)\n",
    "roi_count = R.shape[0] // c.num_rois\n",
    "r2 = np.zeros((roi_count * c.num_rois, r.shape[1]))\n",
    "r2 = r[:r2.shape[0],:r2.shape[1]]\n",
    "r2 = np.reshape(r2, (roi_count, c.num_rois, r.shape[1]))\n",
    "p_cls = []\n",
    "p_regr = []\n",
    "for i in range(r2.shape[0]):\n",
    "    pred = model_classifier_only.predict([F, r2[i: i + 1]])\n",
    "    p_cls.append(pred[0][0])\n",
    "    p_regr.append(pred[1][0])\n",
    "p_cls = np.asarray(p_cls)\n",
    "p_regr = np.asarray(p_regr)\n",
    "p_cls.shape, r2.shape, p_regr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boxes = []\n",
    "w, h, _ = r2.shape\n",
    "for x in range(w):\n",
    "    for y in range(h):\n",
    "        cls_idx = np.argmax(p_cls[x][y])\n",
    "        if cls_idx == len(idx_to_class) - 1:\n",
    "            continue\n",
    "        reg = p_regr[x, y, 4 * cls_idx:4 * (cls_idx + 1)]\n",
    "        params = list(r2[x][y])\n",
    "        params += list(reg / c.classifier_regr_std)\n",
    "        box = keras_frcnn.roi_helpers.apply_regr(*params)\n",
    "        box = list(map(lambda i: i * c.rpn_stride, box))\n",
    "        boxes.append((idx_to_class[cls_idx], p_cls[x][y][cls_idx], box))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes2 = list(sorted(boxes, key=lambda t:t[1], reverse=True))\n",
    "boxes2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = pickle.load(open('../data/config.pickle', 'rb'))\n",
    "new_nb_classes = len(new_config.class_mapping)\n",
    "out = model_classifier_only.layers[-3].output\n",
    "new_out_class = TimeDistributed(Dense(new_nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(new_nb_classes))(out)\n",
    "new_out_regr = TimeDistributed(Dense(4 * (new_nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(new_nb_classes))(out)\n",
    "new_classifer =  [new_out_class, new_out_regr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "num_anchors = len(c.anchor_box_scales) * len(c.anchor_box_ratios)\n",
    "rpn = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(shared_layers, roi_input, c.num_rois, len(c.class_mapping), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "model_rpn.load_weights('../data/model_frcnn.hdf5', by_name=True)\n",
    "model_classifier.load_weights('../data/model_frcnn.hdf5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_config = pickle.load(open('../data/config.pickle', 'rb'))\n",
    "new_nb_classes = len(new_config.class_mapping)\n",
    "out = model_classifier_only.layers[-3].output\n",
    "new_out_class = TimeDistributed(Dense(new_nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(new_nb_classes))(out)\n",
    "new_out_regr = TimeDistributed(Dense(4 * (new_nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(new_nb_classes))(out)\n",
    "new_classifer =  [new_out_class, new_out_regr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model_classifier = Model([img_input, roi_input], classifier)\n",
    "new_model_rpn = Model(img_input, rpn[:2])\n",
    "\n",
    "new_model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "new_model_all.save_weights('../data/model_frcnn_new.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}